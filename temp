from pyspark.sql.functions import col, isnan, when, count
from pyspark.sql.types import StringType

def is_column_Safe_To_cast(df,col_name):
    if not isinstance(df.schema[col_name].dataType,StringType):
        return False
    
    #cldan temp column treat 'NA' ,"NAN","" as null
    clean_col=when(
                col(col_name).isin("NA","NaN",""),
                None
                ).otherwise(col(col_name))
    
    #count non null entreis before cating after cleaning 
    total_valid_before=df.withColumn("temp_cleaned",clean_col).filter(col("temp_cleaned").isNotNull()).count()
    print(f"total_Valid_bfroe {col_name} is {total_valid_before}")
    #cast the column
    try:
        casted_col=clean_col.cast("double")
        total_valid_after= df.withColumn("temp_Casted",casted_col).filter(col("temp_Casted").isNotNull()).count()

    except Exception as e:
        print(f"error casting {col_name} to double:")
        return False
    
    return total_valid_after >= 0.95 *total_valid_before


safe_numeric_col=[]
for col_name in df.columns:
    if is_column_Safe_To_cast(df,col_name):
        print(col_name)
        safe_numeric_col.append(col_name)
print("columns safely castable to double: ",safe_numeric_col)
