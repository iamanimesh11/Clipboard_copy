Great! Now that you've completed message filtering and transformation, let's move to the next step: Persisting messages into a database. This will allow you to store chat history and perform analytics later.


---

Step: Persist Messages to a Database

We'll use PostgreSQL for storing messages. Here's what we'll do:

1. Set up a Database:

Create a messages table with fields for id, user, message, timestamp, and partition.



2. Update Consumer Script:

Modify the consumer to save messages into the database.





---

Database Setup

Create a table in PostgreSQL:

CREATE TABLE chat_messages (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50),
    message TEXT,
    timestamp TIMESTAMP,
    partition INT
);


---

Enhanced Consumer Script

We'll modify the consumer to save messages into the PostgreSQL database.

Install Required Packages

Ensure you have the psycopg2 library installed in your environment:

pip install psycopg2

Script

from kafka import KafkaConsumer
import json
import psycopg2
from psycopg2 import sql


def save_to_db(user, message, timestamp, partition):
    try:
        # Connect to PostgreSQL
        connection = psycopg2.connect(
            dbname="your_database",
            user="your_user",
            password="your_password",
            host="localhost",
            port="5432"
        )
        cursor = connection.cursor()

        # Insert message into the database
        insert_query = sql.SQL("""
            INSERT INTO chat_messages (username, message, timestamp, partition)
            VALUES (%s, %s, %s, %s)
        """)
        cursor.execute(insert_query, (user, message, timestamp, partition))
        connection.commit()

        print("Message saved to database!")

    except Exception as e:
        print(f"Error saving to database: {e}")

    finally:
        if connection:
            cursor.close()
            connection.close()


def consumer():
    consumer = KafkaConsumer(
        'chat-messagessss',
        bootstrap_servers='172.19.165.234:9092',
        value_deserializer=lambda v: json.loads(v.decode('utf-8')),
        group_id='chat-group'
    )
    print("Chat Consumer started. Listening for messages:")

    for message in consumer:
        data = message.value

        user = data['user']
        msg_content = data['message']
        partition = message.partition  # Get the partition
        timestamp = message.timestamp  # Get message timestamp

        # Print received message
        print(f"Received - User: {user}, Message: {msg_content}, Partition: {partition}, Timestamp: {timestamp}")

        # Save to database
        save_to_db(user, msg_content, timestamp, partition)


if __name__ == "__main__":
    consumer()


---

Testing

1. Run PostgreSQL:

Ensure your PostgreSQL server is running and the chat_messages table exists.



2. Start Consumer:

Run the updated consumer script to begin saving messages to the database.



3. Send Messages:

Use the producer script to send messages. Verify that they are stored in the database by querying:


SELECT * FROM chat_messages;




---

Next Steps

Once this works, we can proceed to:

1. Data Analytics:

Perform queries to analyze user activity (e.g., most active users).

Visualize data using tools like Power BI or Tableau.



2. Notification System:

Trigger alerts for specific keywords or conditions.




Would you like to focus on analytics, notifications, or another enhancement next?

