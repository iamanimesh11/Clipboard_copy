ğŸš€ We All Say â€œParquet is Columnarâ€â€¦ But Itâ€™s More Than That!

While working in AWS, I tried converting some CSVs into Parquet just to see if query performance improved â€” and the results surprised me.

Parquet behaves more like a self-optimizing data engine than a storage file.

ğŸ”¥ It uses dynamic encoding â€” and it can switch encoding page by page inside the same column.

Before that makes sense, remember how a Parquet file is structured:

 A file is split into Row Groups (big horizontal chunks of data)
 Each column inside a Row Group becomes a Column Chunk
 And each Column Chunk is made of Pages (the smallest unit Parquet encodes)

Encoding decisions happen at the page level.
This is why Parquet can adapt so aggressively.

Think about how raw data normally looks:


Hereâ€™s the wild part:

â­ Parquet doesnâ€™t pick one encoding per column.

It adapts mid-file, page by page, inside each Column Chunk.

If the first pages in a Row Group compress well with dictionary encoding â†’ great.
If later pages suddenly have high cardinality â†’ Parquet switches strategies seamlessly.

No config. No tuning.
Optimal encoding per page â€” thousands of micro-optimizations inside every file.


# âš¡ Why this matters beyond â€œcolumnar storageâ€

Adaptive, page-level encoding leads to:

 Better compression
 Faster queries
 Lower compute cost
 Page statistics that reveal drift/cardinality changes
 A format that optimizes itself as your data evolves

Parquet isnâ€™t just columnar â€” itâ€™s adaptive, reacting to your data in real time, quietly improving performance without anyone noticing.

